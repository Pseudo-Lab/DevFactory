# Open-WebUI 실행

이번 세션에서는 **Open-WebUI** 리포지토리(https://github.com/open-webui/open-webui)를 참고하여, Docker 컨테이너로 Web UI를 실행하고 Ollama 서버와 연동하는 방법을 실습합니다.

---

## Open-WebUI란?

- **Open-WebUI**: 오픈소스 웹 기반 LLM 인터페이스  
- 깔끔한 React/Next.js 프론트엔드로, 다양한 로컬 LLM 서버(예: Ollama, llama.cpp)와 연동 가능  
- GitHub Repository : https://github.com/open-webui/open-webui

---

## Open-WebUI Docker 이미지 가져오기

```bash
docker pull ghcr.io/open-webui/open-webui:git-1349bc4-cuda
```

- GitHub Container Registry에서 `git-1349bc4-cuda` 태그의 이미지를 가져옵니다.  
    (2025.04.27, 현재 latest 버전 다운로드 오류 발생)

---

## Open-WebUI 컨테이너 실행

```bash
docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway \
       -v open-webui:/app/backend/data --name open-webui --restart always \  
       ghcr.io/open-webui/open-webui:git-1349bc4-cuda
```

- `-d` : 백그라운드(detached) 모드로 컨테이너를 실행합니다.
- `-p 3000:8080` : 호스트의 포트 3000을 컨테이너 내부의 8080 포트와 연결하여, 브라우저에서 `http://localhost:3000`으로 접속 시 Open-WebUI에 접근합니다.
- `--add-host=host.docker.internal:host-gateway` : 컨테이너 내에서 `host.docker.internal` 호스트명을 호스트 머신의 게이트웨이 주소로 매핑하여, Ollama 서버 등 호스트의 서비스에 쉽게 접근할 수 있도록 설정합니다.
- `-v open-webui:/app/backend/data` : `open-webui`라는 named volume을 컨테이너의 `/app/backend/data`에 마운트하여, 캐시된 모델 파일과 사용자 설정 데이터를 영구 저장합니다.
- `--name open-webui` : 컨테이너 이름을 `open-webui`로 지정하여, 이후 명령에서 편리하게 참조할 수 있습니다.
- `--restart always` : 컨테이너가 중단되거나 Docker가 재시작된 경우에도 자동으로 재실행되도록 설정합니다.
- `ghcr.io/open-webui/open-webui:git-1349bc4-cuda` : GitHub Container Registry의 Open-WebUI CUDA 빌드 태그 이미지입니다.

## Web UI 접속 및 사용

- 브라우저에서 [http://localhost:3000](http://localhost:3000) 접속  
- 로그인 진행 -> 임의의 값을 넣어줘도 상관없음
- 좌측 패널에서 **Ollama** 백엔드 선택  
- 다운로드 했던 **gemma3** 모델을 선택 후 프롬프트 입력  
- **Send** 버튼 클릭하여 응답 확인  

## Ollama 모델 추가하기

```bash
docker exec ollama ollama pull gemma3:1b
```

모델 다운로드 이후 브라우저를 새로고침하고 모델이 추가된 것을 확인합니다.

---

**다음 세션**에서는 **Docker Compose**로 Ollama 서버와 Open-WebUI를 함께 띄우는 방법을 배워봅시다.
