# Ollama로 로컬 LLM 실행

이번 세션에서는 Docker를 이용하여 **Ollama 서버**를 띄우고, 로컬 환경에서 대규모 언어 모델(LLM)을 간편하게 실행하는 방법을 실습합니다.

---

## Ollama란?

- **로컬에서 실행 가능한 경량 LLM 서버**
    - ChatGPT 호환 API를 제공해 누구나 자신의 머신에서 손쉽게 LLM을 사용 가능
    - GPU가 있으면 가속 활용 가능, 없으면 CPU로도 실행 가능

- GitHub Repository : https://github.com/ollama/ollama

---

## Ollama Docker 이미지 가져오기

Ollama 팀이 제공하는 [공식 이미지](https://hub.docker.com/r/ollama/ollama) 사용

```bash
docker pull ollama/ollama
```

- 최신 이미지 Pull
- 특정 버전 사용 시:`ollama/ollama:<tag>` 형식으로 지정 가능  

## Ollama 서버 컨테이너 실행

```bash
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama

```

- `-d` : 백그라운드 모드로 실행  
- `--name ollama` : 컨테이너 이름 지정  
- `-v ollama:/root/.ollama` : 모델·설정 파일을 named volume에 영구 저장  
- `-p 11434:11434` : 호스트 포트 11434 ↔ 컨테이너 포트 11434 매핑 (API 접근용)

## Ollama 서버 정상 기동 확인

**컨테이너 상태 확인**  
```bash
docker ps
```

**예상 출력**
```bash
CONTAINER ID   IMAGE             COMMAND                   CREATED          STATUS          PORTS                      NAMES
0cae5a78228c   ollama/ollama     "/bin/ollama serve"       12 seconds ago   Up 12 seconds   0.0.0.0:11434->11434/tcp   ollama
```

## `ollama` 컨테이너 실행 여부 확인
 **API 엔드포인트 접근 테스트**
```bash
curl http://localhost:11434/api/tags
```
- 설치된 모델 목록이 출력되거나 빈 배열(`[]`) 반환  

## Gemma 3 모델 다운로드
- 개인 PC에서도 사용할 수 있도록 가벼운 gemma 3 모델 다운로드  
- 다양한 모델을 다운로드하여 사용 가능

## 모델 선택하기
#### 모델 파라미터 및 크기
>  예시에서는 Gemma 3를 사용하지만, 아래의 모델표를 참고하여, 자신의 컴퓨터 사양에 맞는 모델을 선택할 수 있습니다. (추가 모델은 [여기](https://github.com/ollama/ollama?tab=readme-ov-file#model-library)에서 확인할 수 있습니다.)

- 사양을 모른다면, 작은 모델 선택을 권장합니다.

## 모델 정보

| 모델 이름       | 파라미터 수 | 크기    | 모델 명(코드)                |
|----------------|------------|---------|-------------------------------|
| Gemma 3 (1B)  | 1B         | 815MB   | `gemma3:1b`        |
| Gemma 3 (4B)  | 4B         | 3.3GB   | `gemma3`           |
| Gemma 3 (12B) | 12B        | 8.1GB   | `gemma3:12b`       |
| Gemma 3 (27B) | 27B        | 17GB    | `gemma3:27b`       |
| QwQ           | 32B        | 20GB    | `qwq`              |
| DeepSeek-R1   | 7B         | 4.7GB   | `deepseek-r1`      |
| DeepSeek-R1   | 671B       | 404GB   | `deepseek-r1:671b` |
| Llama 3.3     | 70B        | 43GB    | `llama3.3`         |
| Llama 3.2 (3B)| 3B         | 2.0GB   | `llama3.2`         |
| Llama 3.2 (1B)| 1B         | 1.3GB   | `llama3.2:1b`      |
| Llama 3.2 Vision | 11B     | 7.9GB   | `llama3.2-vision`  |


### bash
```bash
docker exec ollama ollama pull gemma3
```

### REST API
```bash
curl http://localhost:11434/api/pull -d '{
  "model": "gemma3"
}'
```

## Gemma3 모델로 질문하기
### bash
```bash
docker exec ollama ollama generate -m gemma3 -p "Why is the sky blue?"
```

### REST API
```bash
curl -s http://localhost:11434/api/generate -d '{
  "model": "gemma3",
  "prompt": "Why is the sky blue?",
  "options": {
    "num_predict": 10
  }
}'
```

## Ollama 서버 주요 기능

- 모델 다운로드 · 실행 (CLI 명령어, REST API)  
- OpenAI API 호환 서버로 활용 가능  
- 로컬 PC 또는 서버에 자체 LLM 인프라 구축 가능  

---

**다음 세션**에서는 Ollama 위에 **Open-webui**를 띄워 웹 브라우저로 대화형 AI를 사용해 봅시다.
