# Ollama로 로컬 LLM 실행

이번 세션에서는 Docker를 이용하여 **Ollama 서버**를 띄우고, 로컬 환경에서 대규모 언어 모델(LLM)을 간편하게 실행하는 방법을 실습합니다.

---

## Ollama란?

- **로컬에서 실행 가능한 경량 LLM 서버**
    - ChatGPT 호환 API를 제공해 누구나 자신의 머신에서 손쉽게 LLM을 사용 가능
    - GPU가 있으면 가속 활용 가능, 없으면 CPU로도 실행 가능

- GitHub Repository : https://github.com/ollama/ollama

---

## Ollama Docker 이미지 가져오기

Ollama 팀이 제공하는 [공식 이미지](https://hub.docker.com/r/ollama/ollama) 사용

```bash
docker pull ollama/ollama
```

- 최신 이미지 Pull
- 특정 버전 사용 시:`ollama/ollama:<tag>` 형식으로 지정 가능  

## Ollama 서버 컨테이너 실행

```bash
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama

```

- `-d` : 백그라운드 모드로 실행  
- `--name ollama` : 컨테이너 이름 지정  
- `-v ollama:/root/.ollama` : 모델·설정 파일을 named volume에 영구 저장  
- `-p 11434:11434` : 호스트 포트 11434 ↔ 컨테이너 포트 11434 매핑 (API 접근용)

## Ollama 서버 정상 기동 확인

**컨테이너 상태 확인**  
```bash
docker ps
```

**예상 출력**
```bash
CONTAINER ID   IMAGE             COMMAND                   CREATED          STATUS          PORTS                      NAMES
0cae5a78228c   ollama/ollama     "/bin/ollama serve"       12 seconds ago   Up 12 seconds   0.0.0.0:11434->11434/tcp   ollama
```

## `ollama` 컨테이너 실행 여부 확인
 **API 엔드포인트 접근 테스트**
```bash
curl http://localhost:11434/api/tags
```
- 설치된 모델 목록이 출력되거나 빈 배열(`[]`) 반환  

## Gemma 3 모델 다운로드
- 개인 PC에서도 사용할 수 있도록 가벼운 gemma 3 모델 다운로드  
- 다양한 모델을 다운로드하여 사용 가능

### bash
```bash
docker exec ollama ollama pull gemma3
```

### REST API
```bash
curl http://localhost:11434/api/pull -d '{
  "model": "gemma3"
}'
```

## Gemma3 모델로 질문하기
### bash
```bash
```

### REST API
```bash
curl -s http://localhost:11434/api/generate -d '{
  "model": "gemma3",
  "prompt": "Why is the sky blue?",
  "options": {
    "num_predict": 10
  }
}'
```

## Ollama 서버 주요 기능

- 모델 다운로드 · 실행 (CLI 명령어, REST API)  
- OpenAI API 호환 서버로 활용 가능  
- 로컬 PC 또는 서버에 자체 LLM 인프라 구축 가능  

---

**다음 세션**에서는 Ollama 위에 **Open-webui**를 띄워 웹 브라우저로 대화형 AI를 사용해 봅시다.
