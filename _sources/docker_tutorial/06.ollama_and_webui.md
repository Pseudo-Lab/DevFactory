# Ollama + WebUI (Docker Compose)

이번 세션에서는 **Docker Compose**를 사용해 **Ollama 서버**와 **Open-WebUI**를 동시에 실행하는 전체 스택을 구성합니다.  
단일 `docker compose up` 명령만으로 LLM 서버와 웹 인터페이스가 함께 기동됩니다.

---

## 디렉터리 구조

```
└── ollama-webui/
    └── docker-compose.yaml
```

---

## `docker-compose.yaml` 작성

```yaml
version: "3.8"

services:
  ollama:
    image: ollama/ollama
    container_name: ollama
    volumes:
      - ollama-data:/root/.ollama
    ports:
      - "11434:11434"

  open-webui:
    image: ghcr.io/open-webui/open-webui:git-1349bc4-cuda
    container_name: open-webui
    depends_on:
      - ollama
    ports:
      - "3000:8080"
    volumes:
      - open-webui-data:/app/backend/data
    extra_hosts:
      - "host.docker.internal:host-gateway"
    restart: always

volumes:
  ollama-data:
  open-webui-data:
```

- **ollama**  
  - `ollama/ollama` 이미지를 사용해 로컬 LLM 서버 실행  
  - 모델과 설정을 `ollama-data` 볼륨에 영구 저장  
  - 포트 `11434`로 API 요청 수신  

- **open-webui**  
  - Open-WebUI 인터페이스를 실행하는 컨테이너  
  - 호스트 `3000` 포트를 컨테이너 `8080`으로 매핑  
  - 사용자 데이터 및 캐시를 `open-webui-data` 볼륨에 저장  
  - `depends_on`으로 Ollama 서비스 기동을 보장  
  - `extra_hosts` 설정으로 호스트 접근 지원  

---
## 기존 리소스 정리
`docker compose`를 통한 스택을 실행하기 전에 기존에 실행 중인 컨테이너들을 정리합니다.

```bash
docker stop ollama open-webui
docker rm ollama open-webui
```

## 전체 스택 실행

```bash
# Compose 파일이 위치한 디렉터리에서
docker compose up -d
```

- `-d`: 백그라운드 실행

---

## 서비스 상태 확인

```bash
docker compose ps
```

- `ollama`와 `open-webui` 컨테이너가 **Up** 상태인지 확인합니다.

---

## 서비스 로그 확인

```bash
docker compose logs -f
```

- `-f` : 실시간(follow) 로그 스트리밍  

## Web UI 접속

- 브라우저에서 [http://localhost:3000](http://localhost:3000) 접속  
- 다운로드된 LLM 모델(예: `gemma3`)을 선택 후 프롬프트 입력 및 **Send** 클릭  

---

## 스택 중지 및 정리

```bash
docker compose down --volumes
```

- `--volumes`: 정의된 볼륨까지 함께 삭제 \
  ※ 주의 : 볼륨을 삭제할 경우 다운로드한 모델까지 같이 삭제됨

---

> **🎉축하합니다!**  
> 이로써 Docker를 이용하여 **로컬 ChatGPT 환경**을 성공적으로 구축하였습니다.
